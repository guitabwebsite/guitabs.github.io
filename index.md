# GuiTabs
### an automatic guitar transcription service written by David Hofferber, Joseph Eligon, Joshua Fields

### Contact Us  
{dch, JosephEligon2019, joshua.fields }@u.northwestern.edu

part of Machine Perception of Audio and Music with Professor Bryan Pardo at Northwestern University

## Synopsis of Work

### Motivation
People want to be able to play music, and the majority of the guitar repertoire is only available as audio

Manual transcription can often take a long time and is also difficult for beginners

### Problems
Most transcriptions needs to be done based on the audio because transcriptions are not commonly available. However, pitch tracking is a difficult task in itself

Even discounting the different positions your hand can be in while playing notes on the guitar, there are still five or six different ways to play every note

Even though the notes are important, the fingerings are probably the most important part of the song in order to be viable to play. However, due to the complexity of the fingering, much of how the fingerings are determined depend upon complex distance metrics

### Pipeline
1. The user indicates an audio file to transcribe
2. We run CREPE on the audio file, determining new notes based off of having a sufficient change in Hz. If CREPE is not confident for too long, we also terminate the current note.
3. Using the note list generated by CREPE, we identify an optimal fingering by performing a shortest path algorithm on a graph representation of the finger pattern.
4. Using the optimal fingerings, we use Muse Score in order to transcribe the notes in tablature, and generate a pdf of it

![Flowchart of our pipeline](https://raw.githubusercontent.com/guitabwebsite/guitabwebsite.github.io/master/images/pipeline.png)  


### Details on Usage of Crepe

CREPE runs a Deep Convolutional Neural Network on 16-bit depth audio files (currently only WAV) and produces some data for us. It generates that data and puts it into a spreadsheet with the columns timestamp (s), frequency (Hz), and Confidence. Timestamp is the time in the audio file, taken every 0.01 seconds, frequency is the pitch CREPE interprets at that timestamp, and confidence is CREPE's certainty that it is correct.

We generate this spreadsheet for our audio files and use the data to produce a list of tuples in the form (notes, durations). In order to do this, we find areas where CREPE has a 60% or higher confidence and average similar frequencies in that range. Once we run into sufficiently different frequencies or areas of low confidence, we add that note and its duration to the final note list. 

Once we have our notes and durations, we make a few optimizations. We check that the pitches of each note are within a certain range such that we don't get unreasonably high or low notes that may happen due to artifacting. We also deal with a common issue in pitch tracking where the pitch tracker reads one note as two, an octave apart. Although there could be false positives, we combine notes that are an octave apart when one note is sufficiently shorter in duration than the other.

### Details on Fingering Algorithm

We initially parse our song with _k_ notes into a graph structure of _k+2_ layers, where each vertex is a way to play the _i<sup>th</sup>_ note.

We then connect each vertex layer to the next vertex layer with directed edges, where weights are assigned by a distance metric between note tuples. Our note tuples represent a node structure from which the way that note is played can be extracted. Due to this, each note tuple contains the following information: the string it is played on, the fret it is played on, the finger used to play the note, the position of the player's hand (i.e. what fret the index finger is closest to, even if not used to play a note), and an occurrence number which states where this particular note occurs in the song.

Our distance metric takes in two note tuples at a time, and uses a variety of cases to determine what edge weight to assign to the edge between two notes. The most impactful results come from the position of your hand in between the two notes, resulting in our shortest path penalizing large hand shifts on the fretboard unless necessary. There are numerous other cases the distance metric takes into consideration, mostly involving the interaction between finger placement relative to the strings notes are played on.

Once we have our finalized graph, we perform Dijkstra's Algorithm to find the shortest path through the graph based on the weighted edges, which returns the fingerings which require the least movement based on our distance metrics.

![Visualization of graph from algorithm](https://raw.githubusercontent.com/guitabwebsite/guitabwebsite.github.io/master/images/algorithmgraph.png)  
The vertex layers of our song, where _v<sub>s</sub>_ is a special node representing the start of our graph structure and _v<sub>E</sub>_ Â¸ is a special node representing the end of our graph structure.


### Results
We were able to successfully transcribe audio into notated guitar tablature. However, that audio had to be monophonic, have very little noise, and be of 16-bit depth. The results were also not always perfect, as a few notes were sometimes an octave off due to our pitch tracker CREPE having trouble determining the octave. Although it should be noted that pitch trackers in general have this issue. Our fingering algorithm will often put emphasis of shifting hand positions and using the pinky instead of using the ring finger and shifting at a later time. This can be a bit uncomfortable for many guitarists.

![Output using Lilypond for tablature](https://raw.githubusercontent.com/guitabwebsite/guitabwebsite.github.io/master/images/lilyTab.png)  
LilyPond tab generation

![Output using GuiTabs for tablature](https://raw.githubusercontent.com/guitabwebsite/guitabwebsite.github.io/master/images/GuiTab.png)  
GuiTab tab generation

### Evaluation Metrics
We evaluated our results based on their pitch accuracy to the actual notes. This was done through human feedback, where the user told us whether or not the pitches sound right.

We evaluated our results based on the fingerings ease of playability. This was done by having several guitar players play the generated tablature and give feedback on how natural it was for them to play it.

## Additional Information

### Installation Instructions
Before running GuiTabs, you must satisfy the following requirements:

1. Install Crepe. To do so, run the following:
```
$ pip install --upgrade tensorflow 
$ pip install crepe
```
2. Install MuseScore3
3. Add the bin folder of Muse Score 3 to your environment variables

### Related Work
Bello and Monti in _Techniques for Automatic Music Transcription_ give a rough blueprint of producing audio transcriptions from audio and some methods that we have not endeavored to use in this project.

Dlabal and Wedeen in _Generating Sheet Music From Audio Files_ give their approach for going from audio to notes and give some detailed note-detection information

Barbancho, et. Al in _Automatic Transcription of Guitar Chords and Fingering_ discusses a way of feature extraction from many guitar samples to glean useful metrics for potential fingerings.

Tuohy and Walter describe a method to generate guitar tabs for audio including chords, but do not have fingerings.

### Future Work
In the future, we could fine tune weights of fingering graph or use an alternative machine learning approach, as well as integrate beat onsets in pitch tracking.

In an ideal world, we would make GuiTabs capable of polyphonic input.





